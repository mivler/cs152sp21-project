<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="styles.css">
    <title>Neural Networks Proposal</title>
</head>
<body>
    <div class="title-bar"> 
        <h1>pAIcasso</h1>
        <h2>Generating Classic and Quirky Works With Neural Networks</h2>
    </div>

    <div class="content-box hidden pitch">
        <h1>Artificial Artist</h1>
        <br>

        <img src=Cubism_example.jpg>

        <h3>Project Description</h3>
        <p>
            I will create a generative adversarial network to train two models.
            The first model will work to create works of art based on a given input and category.
            The second will be used to train the first, categorizing the artifically created artwork,
            and helping to adjust the first until it can create artwork that is highly likely to fall
            within a specific art discipline.
            <br>
            This project will require either finding or creating a dataset of paintings that fall
            within a particular artistic category. Then the creation of a classifier. This classifier
            will aid in determining the classification of the artwork.
            <br>
            The hope is to deploy the project such that allows the user to input a category and get an 
            original work of art relevent to that category back.
        </p>

        <h3>Project Goals</h3>
        <ul>
            <li>Create or find a categorically labeled artwork database.</li>
            <li>Train a classifier for these categories.</li>
            <li>Train a discriminator model that distinguish between categories and between real or fake art.</li>
            <li>Train a generator model that can create original images.</li>
        </ul>
    </div>    

    
    <div class="content-box hidden intro">
        <h1>pAIcasso</h1>
        <br>

        <h3>Group Members: Renae Tamura, Matthew Ivler</h3>
       
        <p>
            <em>Introductory Paragraph:</em>
            Our work training this generative art model not only yields an artificial artist, 
            but gives insights into the potentially arbitrary nature by which society classifies artistic works 
            by displaying an understanding of various styles through its capacity for mimicry and 
            contextualization of content through its caption creation.
            <br><br>
            <em>Background Paragraph:</em>
            Generating realistic and unique images specifically from the input is difficult
            for the neural network to do, since it does not have a sense of what the work
            should look like, and it may take the shortcut of sticking to a singular output
            that happens to receive good feedback. 
            <br><br>
            <em>Transition Paragraph:</em>
            To overcome the difficulties discussed above, we plan to use multiple neural networks
            to train the output for realistic looking images. In addition, we will convert the output
            back to the input with a reverse process, which will ensure that solutions cannot be reused.
            <br><br>
            <em>Details Paragraph:</em>
            Our initial challenge was creating a dataset of images that appropriately aligned with each
             category such that we could properly develop our necessary GAN models, a discriminator model
            for classification/artificial art detection and a generator model for creation of properly classified
            art with a relevant name/caption.
            <br><br>
            <em>Assessment Paragraph:</em>
            Our model creates original works of art that, when judged by humans, were classified as the 
            appropriate style, and as expected, the captions associated with the works indicate an understanding
            of the content depicted in the work, showing there were in fact relevant consistencies 
            throughout the works in a given style.
            <br><br>
            <em>Ethics Discussion:</em>
            Ethically, we must consider the potential effects our project might have on the art community - 
            specifically - when new art can be generated in a matter of seconds. We will further this discussion
            in considering who the “real” artist is in a work produced by a neural network. 
        </p>
    </div>


    <div class="content-box lit-review hidden">
        <h1>pAIcasso</h1>
        <br>
        <h3> 
            Literature Review
        </h3>

        <p> 
            <em> <a href="https://www.weareworldquant.com/en/thought-leadership/generating-art-from-neural-networks/">
                Generating Art From Neural Networks by Tejesh Kinariwala
            </a></em>
            <br>
            The GAN created by Ian Goodfellow was able to create original works of art, 
            even producing and printing the Portrait of Edmond Belamy in 2018.
            The GAN lets the discriminator model maximize the likelihood that real images “pass through”
            the model while synthetic ones don’t, and lets the generator minimize this likelihood.
            This process requires balancing the development of each model in order to have sustainable and realistic
            growth of either of them. 


        </p>
        <br><br>

        <p> 
            <em> <a href="https://arxiv.org/pdf/1508.06576v2.pdf">
                A Neural Algorithm of Artistic Style by Leon Gatys, Alexander Ecker, and Matthias Bethge
            </a></em>
            <br>
            The neural network described in this paper separates style and content and is able to combine
            the two to create new stylistic images. Style is interpreted through the texture in the painting,
            to prevent from the neural network interpreting style through the placement of objects in the artwork.
            In this paper, we see a discussion about balancing the content and the style of the artwork produced,
            to ensure that it is recognizable to the photograph and the style. 
        </p>
        <br><br>
        
        <p> 
            <em> <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf">
                Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks by Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros
            </a></em>
            <br>
            This paper discusses a method for training a neural network to translate images from one
            set to another without previously having labeled data. To do this, a mapping would need
            to be learned from the initial image to the new category, and then an inverse function
            would be used to evaluate that mapping. The loss function was computed between the
            original image and the original image created from the created image. 
        </p>
        <br><br>
        
        <p> 
            <em> <a href="https://thegradient.pub/playing-a-game-of-ganstruction/">
                The Gradient - Playing a game of GANstruction, Helena Sarin
            </a></em>
            <br>
            This is an article written by an artist that uses neural networks to create some of her works.
            She gives descriptions about the different types of GANs that can be used, and why they are effective
            in different scenarios. All of these discussions pertain specifically to art, and address a bit of
            the philosophical questions we asked about who the artist is in this scenario. 
        </p>
        <br><br>
    </div>

    <div class="content-box update hidden">
        <h1>pAIcasso</h1>
        <br>
        <h3> 
            Project Update
        </h3>
        <br>
        <br>
        <h4>
            Software and Dataset
        </h4>
        <p>We will be using PyTorch, Jupyter Notebooks, and potentially CycleGAN to complete this project</p>
        <p>We will create the dataset with Python and APIs to scrape the web for images related to particular art choices</p>
        
        <br>
        
        <h4>
            The NN Design
        </h4>
        <p> We will be using a GAN consisting of a generator model and a discrimiator model. We will shrink our dataset to 256x256
            pixel images which we will pass in to the generator as 3-channel images with an additional "Desired category" input.
            The generator will be a Convolutional Neural Network and will similarly output a 3-channel image of the same dimensions. 
            This generator output, along with some of the art/training inputs will be passed into the discriminator, another Convolutional
            Neural Network. This discriminator model will output a vector of dimensions (n+1, 1) where n is the number of categories of art,
            and the +1 stands in for a "fake" category that does not fit in with the other artistic classifications. 
        </p>

    </div>

    <div class="content-box Intro-related-works hidden">
        <h1>pAIcasso</h1>
        <br>
        <h3> 
            Introduction/Related Works
        </h3>
        <br>
        <br>
        <h4>
            Introduction
        </h4>
        <p>Our work training this generative art model yields more than original works of art. By displaying an understanding of various styles through its capacity for stylistic mimicry and contextualization of content through caption creation, it gives insights into the potentially arbitrary nature by which society classifies artistic works. Through the generation of artificial two-dimensional works of art and associated captions, we are able to develop a sense of what characteristics of the artwork are intrinsic to that particular style. In addition, the artificial artist produces original works based on a predetermined style and the input image’s content features. </p>
        <p>We plan to use multiple neural networks and CycleGAN to generate new images that can be classified as one of the following artistic categories: Impressionism, Cubism, Contemporary, Modern, Abstract, Surrealism, Xieyi, Gongbi, Min-hwa, Mughal, and Kente. Additionally, we will use similar methods to generate captions for each of the generated images. Using CycleGAN allows us to do this without labeled training data and to train on sets of styles, rather than individual artistic works. </p>
        <p>Our initial challenge is creating a dataset of images that appropriately aligned with each category with which we could properly train our necessary GAN models – a discriminator model for classification/artificial art detection and a generator model for creation of properly classified art with a relevant name/caption. The dataset is created through scraping the web for images in each of the artistic categories, then checking them by hand to ensure they are appropriate inputs (no repeats, ensure images of art, not photos of exhibits, etc…). Finally, we alter the dimensions of each input to be 256x256 pixels, which speeds up training and keeps everything consistent for our neural network’s sizing values. We utilize three convolutional neural networks for creation, classification, and input-feature decomposition, respectively. Training and tweaking hyperparameters of the classification, input-feature decomposition, and creation models, we check the visual acuity and stylistic applications of the creation model.</p>
        <p>In the end, our model creates original works of art that, when judged by humans, were classified as the appropriate style, and as expected, the captions associated with the works indicate an understanding of the content depicted in the work. This shows there are, in fact, relevant consistencies throughout the works in a given style.</p>            
        <br>
        
        <h4>
            Literature Review
        </h4>
        <p> The idea of translating photos into works of art with predetermined styles is not a unique problem. The baseline ideas of image generation and style transformation can be applied to a number of other problems, from simply generating a realistic image of a horse, to changing that horse into a realistic zebra. So, as would be expected, there has been much research for solutions and improvement in the area. </p>
        <p>Ian Goodfellow was the first to solve the image generation problem, when he and other researchers introduced Generative Adversarial Networks (GANs) in 2014 [1]. Previously, neural networks could be trained to recognize objects and things, but their concept of what that “thing” looked like was far from realistic to the human eye. GANs utilize two neural networks - a generator and a discriminator - which learn together to produce images that are indistinguishable from real photos. Essentially the generator produces an image, and the discriminator determines if it thinks that it is real. Then, both neural networks are updated and in turn have learned more from each other about what might constitute a real vs. fake image. To this day, GANs are some of the best at generating images through neural networks, and have been continuously used to progress solutions to the problems outlined above.</p>
        <p> In 2015, Leon Gatys, Alexander Ecker and Matthias Bethge separated style from content in pictures, leading to what is known as Neural Style Transfer. In their paper, they discuss their method for taking two images, and generating a new picture that contains the content of the first in the style of the second. Their loss function was defined to find weights to minimize the loss of style and content separately. The result was many pictures that were clearly in the style of specific paintings, such as The Starry Night by Vincent van Gogh, Der Schrei by Edvard Munch, and Femme nue assise by Pablo Picasso. See [2] for examples.</p>
        <p>The work done by [2] was groundbreaking at the time, but it still had its drawbacks. Neural Style Transfer was designed to work between two distinct images, and it could not map entire sets to each other. Pix2Pix [6] became a well-known image-to-image translation tool, but it required large amounts of labeled data, something that is hard to come by for many of the problems outlined above. This is where CycleGAN comes in. CycleGAN [3] was developed by Zhu et al. in an attempt to tackle image-to-image translation without labeled data. They did this by using two GANs, which allowed them to generate realistic images that were also directly mapped to the original image. That is, for an input image in set X they found a function F: X -> Y that translated the input into an image in the set Y. Then, to ensure that the generated image was “from” the input image, they found another function G: Y -> X and transformed it back. The loss function was centered around the difference between the input image and the generated image from function G. This allowed for a neural network to make connections between sets of things without labeled data on which the neural networks could have been trained. [3] provides a number of examples for how CycleGAN can be used, including style transfer in artwork, horses to zebras, and summer to winter landscapes.</p>
        <p>Separate from the work we will be doing, it should be mentioned that CycleGAN is not just used for image translation. It is also used by artists to generate new and unique works that are satisfyingly intriguing and not what you might expect. Helena Sarin [4] is one such artist, who uses GANs as her medium. In our exploration of image artwork style translation, we hope to also encounter images that sometimes might not be what we wanted or expected, but are strange, new, and interesting nonetheless.</p>
    </div>

    <div class="content-box update-2 hidden">
        <h1>pAIcasso</h1>
        <br>
        <h3> 
            Update 2
        </h3>
        <p>
            We have collected 300 photos for each category in our dataset and are parsing through them to unsure they fit our standards.
            That means, no duplicates, images themselves, not pictures of images or exhibits, and actual art, not junk from the search results.
            <br><br>
            So far our main issue is cleaning the dataset, and figuring out how to do a CNN.
            <br><br>
            We are aiming for an A.
        </p>
    </div>

    <div class="content-box Discussion-Outline hidden">
        <h1>pAIcasso</h1>
        <br>
        <h3> 
            Discussion
        </h3>
        <h4>
            Introduction
        </h4>
        <p>Our work training this generative art model yields more than original works of art. By displaying an understanding of various styles through its capacity for stylistic mimicry and contextualization of content through caption creation, it gives insights into the potentially arbitrary nature by which society classifies artistic works. Through the generation of artificial two-dimensional works of art and associated captions, we are able to develop a sense of what characteristics of the artwork are intrinsic to that particular style. In addition, the artificial artist produces original works based on a predetermined style and the input image’s content features. </p>
        <p>We plan to use multiple neural networks to generate new images that can be classified as one of the following artistic categories: Impressionism, Cubism, Contemporary, Modern, Abstract, Surrealism, Xieyi, Gongbi, Min-hwa, Mughal, and Kente. Additionally, we will use similar methods to generate captions for each of the generated images. </p>
        <p>Our initial challenge is creating a dataset of images that appropriately aligned with each category with which we could properly train our necessary GAN models – a discriminator model for classification/artificial art detection and a generator model for creation of properly classified art with a relevant name/caption. The dataset is created through scraping the web for images in each of the artistic categories, then checking them by hand to ensure they are appropriate inputs (no repeats, ensure images of art, not photos of exhibits, etc…). Finally, we alter the dimensions of each input to be 256x256 pixels, which speeds up training and keeps everything consistent for our neural network’s sizing values. We utilize three convolutional neural networks for creation, classification, and input-feature decomposition, respectively. Training and tweaking hyperparameters of the classification, input-feature decomposition, and creation models, we check the visual acuity and stylistic applications of the creation model.</p>
        <p>In the end, our model creates original works of art that, when judged by humans, were classified as the appropriate style, and as expected, the captions associated with the works indicate an understanding of the content depicted in the work. This shows there are, in fact, relevant consistencies throughout the works in a given style.</p>            
        <br>
        
        <h4>
            Literature Review
        </h4>
        <p> The idea of translating photos into works of art with predetermined styles is not a unique problem. The baseline ideas of image generation and style transformation can be applied to a number of other problems, from simply generating a realistic image of a horse, to changing that horse into a realistic zebra. So, as would be expected, there has been much research for solutions and improvement in the area. </p>
        <p>Ian Goodfellow was the first to solve the image generation problem, when he and other researchers introduced Generative Adversarial Networks (GANs) in 2014 <a href="https://www.weareworldquant.com/en/thought-leadership/generating-art-from-neural-networks/">[1]</a>. Previously, neural networks could be trained to recognize objects and things, but their concept of what that “thing” looked like was far from realistic to the human eye. GANs utilize two neural networks - a generator and a discriminator - which learn together to produce images that are indistinguishable from real photos. Essentially the generator produces an image, and the discriminator determines if it thinks that it is real. Then, both neural networks are updated and in turn have learned more from each other about what might constitute a real vs. fake image. To this day, GANs are some of the best at generating images through neural networks, and have been continuously used to progress solutions to the problems outlined above.</p>
        <p> In 2015, Leon Gatys, Alexander Ecker and Matthias Bethge separated style from content in pictures, leading to what is known as Neural Style Transfer. In their paper, they discuss their method for taking two images, and generating a new picture that contains the content of the first in the style of the second. Their loss function was defined to find weights to minimize the loss of style and content separately. The result was many pictures that were clearly in the style of specific paintings, such as The Starry Night by Vincent van Gogh, Der Schrei by Edvard Munch, and Femme nue assise by Pablo Picasso. See <a href="https://arxiv.org/pdf/1508.06576v2.pdf">[2]</a> for examples.</p>
        <p>The work done by <a href="https://arxiv.org/pdf/1508.06576v2.pdf">[2]</a> was groundbreaking at the time, but it still had its drawbacks. Neural Style Transfer was designed to work between two distinct images, and it could not map entire sets to each other. Pix2Pix <a href="https://phillipi.github.io/pix2pix/">[6]</a> became a well-known image-to-image translation tool, but it required large amounts of labeled data, something that is hard to come by for many of the problems outlined above. This is where CycleGAN comes in. CycleGAN <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf">[3]</a> was developed by Zhu et al. in an attempt to tackle image-to-image translation without labeled data. They did this by using two GANs, which allowed them to generate realistic images that were also directly mapped to the original image. That is, for an input image in set X they found a function F: X -> Y that translated the input into an image in the set Y. Then, to ensure that the generated image was “from” the input image, they found another function G: Y -> X and transformed it back. The loss function was centered around the difference between the input image and the generated image from function G. This allowed for a neural network to make connections between sets of things without labeled data on which the neural networks could have been trained. <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf">[3]</a> provides a number of examples for how CycleGAN can be used, including style transfer in artwork, horses to zebras, and summer to winter landscapes.</p>
        <p>Separate from the work we will be doing, it should be mentioned that CycleGAN is not just used for image translation. It is also used by artists to generate new and unique works that are satisfyingly intriguing and not what you might expect. Helena Sarin <a href="https://thegradient.pub/playing-a-game-of-ganstruction/ ">[4]</a> is one such artist, who uses GANs as her medium. In our exploration of image artwork style translation, we hope to also encounter images that sometimes might not be what we wanted or expected, but are strange, new, and interesting nonetheless.</p>

        <h4>
            Methods Outline
        </h4>
        <p>
            To create our neural network and analyze its performance, we will use a variety of tools for implementation, training and analysis. Doing so will allow us to create an implementation of CycleGAN specific to our project.
            We will use Pytorch and Jupyter notebooks to do most construction of the model, which will have attributes similar to the repository found at https://github.com/aitorzip/PyTorch-CycleGAN. 
            After building the network, we will train on our own dataset of images collected from the Bing API and consisting of 11 categories: Impressionism, Cubism, Contemporary, Modern, Abstract, Surrealism, Xieyi, Gongbi, Min-hwa, Mughal, and Kente. 
            Each category had just under 300 images before cleaning; this number ranges from 200-275 after cleaning, and will be closer to 200 for the actual training of each category. 
            For an analysis of the CycleGAN we create, we will use matplotlib to graph and analyze the accuracy and values obtained from our Minimax Loss function. 
            This will give us insight into parameters that can be manually changed to improve performance, and the general effectiveness of our neural network’s purposes.
        </p>

        <h4>
            Discussion Outline
        </h4>
        <p> Evaluating the performance of our model is relatively straightforward. We simply want to see how many fake/generated images classify as real/authentic works of art. Though this method sounds good in practice, it could ultimately be skewed by the generator model developing at a faster pace than the discriminator model. Since the goal is to make a model that can produce art in a given style, the similarity of the art pieces can also be determined by eye. That said, we continued to monitor the accuracy of images that are classified as real or fake to ensure that on epochs where the discriminator is backpropagated, it gets better over time, and on epochs where the generator is backpropagated the classifier gets worse. </p>
        <p> By evaluating these patterns on a per epoch basis we can also distinguish which/if one model is progressing significantly faster than the other, allowing us to determine if the learning rate of a given model could be reduced. Similarly, by comparing images generated to those from our dataset, we could visually evaluate the similarity to the dataset and consistency between generated images.</p>
        <p> Relative to other similar work in the discipline, our images are produced without an emphasis on adding style to a context image with neural style transfer, rather the creation of a generic image that falls within a stylistic discipline.</p>
        <p> It is important to understand the implications of this tool. Firstly there is the question of whether our AI is merely a tool for us to create, or if we would be wrongly taking credit for its work.</p>
        <p> Finally, there are ethical implications to the art created by a machine that attributes no meaning to it. One of the styles we looked into was Kente, which is often produced on textiles and often attributes meaning to different colors and patterns. Understanding this, even if replicated well, it seems unethical to truly consider the work as Kente if its creator does not understand nor acknowledge the meanings of the components that create this style.</p>
    
    </div>

    <div class="content-box Rough-Draft">
        <h1>pAIcasso Rough Draft</h1>
        <br>
        <h4>
            Introduction
        </h4>
        <p>Our work training this generative art model yields more than original works of art. By displaying an understanding of various styles through its capacity for stylistic mimicry, it gives insights into the potentially arbitrary nature by which society classifies artistic works. Through the generation of artificial two-dimensional works of art, we are able to develop a sense of what characteristics of the artwork are intrinsic to that particular style. In addition, the artificial artist produces original works based on a predetermined style and randomized input noise.</p>
        <p>With a DCGAN, we use multiple neural networks to generate new images that can be classified as one of the following artistic categories: Impressionism, Cubism, Contemporary, Modern, Abstract, Surrealism, Xieyi, Gongbi, Min-hwa, Mughal, and Kente. Our initial challenge was creating a dataset of images that appropriately aligned with each category with which we could properly train our necessary GAN models – a discriminator model for classification/artificial art detection and a generator model for creation of properly classified art. The dataset is created through scraping the web for images in each of the artistic categories, then checking them by hand to ensure they are appropriate inputs (no repeats, ensure images are of art, not photos of exhibits, etc…). Finally, we alter the dimensions of each input to be 256x256 pixels through resizing and randomized crop transforms, keeping everything consistent for our neural network’s sizing values. We utilize two convolutional neural networks, one for art creation and the other for art classification. Training and tweaking hyperparameters of the classification and creation models, we check the visual acuity and stylistic applications of the creation model.</p>
        <p>WE'LL SEE</p>
        <br>

        <h4>
            Literature Review
        </h4>
        <p>Though generic GANs have been seen commonly in discipline, we often find style analysis in neural networks being applied to neural style transfer. The idea of translating photos into works of art with predetermined styles is not a unique problem. The baseline ideas of image generation and style transformation can be applied to a number of other problems, from simply generating a realistic image of a horse, to changing that horse into a realistic zebra. So, as would be expected, there has been much research for solutions and improvement in the area.</p>
        <p>Ian Goodfellow was the first to solve the image generation problem, when he and other researchers introduced Generative Adversarial Networks (GANs) in 2014 [1]. Previously, neural networks could be trained to recognize objects and things, but their concept of what that “thing” looked like was far from realistic to the human eye. GANs utilize two neural networks - a generator and a discriminator - which learn together to produce images that are indistinguishable from real photos. Essentially the generator produces an image, and the discriminator determines if it thinks that it is real. Then, both neural networks are updated and in turn have learned more from each other about what might constitute a real vs. fake image. To this day, GANs are some of the best at generating images through neural networks, and have been continuously used to progress solutions to the problems outlined above.</p>
        <p>In 2015, Leon Gatys, Alexander Ecker and Matthias Bethge separated style from content in pictures, leading to what is known as Neural Style Transfer. In their paper, they discuss their method for taking two images, and generating a new picture that contains the content of the first in the style of the second. Their loss function was defined to find weights to minimize the loss of style and content separately. The result was many pictures that were clearly in the style of specific paintings, such as The Starry Night by Vincent van Gogh, Der Schrei by Edvard Munch, and Femme nue assise by Pablo Picasso. See [2] for examples.</p>
        <p>The work done by [2] was groundbreaking at the time, but it still had its drawbacks. Neural Style Transfer was designed to work between two distinct images, and it could not map entire sets to each other. Pix2Pix [6] became a well-known image-to-image translation tool, but it required large amounts of labeled data, something that is hard to come by for many of the problems outlined above. This is where CycleGAN comes in. CycleGAN [3] was developed by Zhu et al. in an attempt to tackle image-to-image translation without labeled data. They did this by using two GANs, which allowed them to generate realistic images that were also directly mapped to the original image. That is, for an input image in set X they found a function F: X -> Y that translated the input into an image in the set Y. Then, to ensure that the generated image was “from” the input image, they found another function G: Y -> X and transformed it back. The loss function was centered around the difference between the input image and the generated image from function G. This allowed for a neural network to make connections between sets of things without labeled data on which the neural networks could have been trained. [3] provides a number of examples for how CycleGAN can be used, including style transfer in artwork, horses to zebras, and summer to winter landscapes.</p>
        <p>Separate from the work we will be doing, it should be mentioned that CycleGAN is not just used for image translation. It is also used by artists to generate new and unique works that are satisfyingly intriguing and not what you might expect. Helena Sarin [4] is one such artist, who uses GANs as her medium. In our exploration of image artwork style translation, we hope to also encounter images that sometimes might not be what we wanted or expected, but are strange, new, and interesting nonetheless.</p>

        <h4>
            Methods
        </h4>
        <p>To create our neural network and analyze its performance, we will use a variety of tools for implementation, training and analysis. We use Pytorch and Jupyter notebooks to do most construction of the model, utilizing some previously positive concepts and GAN implementations such as DCGAN. In order to train the network we created a dataset of images collected from the Bing API and consisting of 11 categories: Impressionism, Cubism, Contemporary, Modern, Abstract, Surrealism, Xieyi, Gongbi, Min-hwa, Mughal, and Kente. Each category originally had 300 images before cleaning; this number ranges from 79-266 after cleaning. This cleaning process consisted of removing duplicates picked up in the dataset as well as pictures of artwork/exhibits. After applying transforms to the dataset, we ultimately had a dataset of 18,996 training images and 8,136 validation images.</p>
        <p>Since a GAN utilizes two models, we first created a discriminator model. Without the corresponding generator model, the job of the discriminator was simply to categorize art from the dataset we had. Through the categorization of this art, we tweaked the architecture of the Convolutional Neural Network by adding layers, changing the pooling methods, and tweaking the learning rate, among other things. We continued this process until we felt that, given our small set of epochs to train on and batch size of 32 images, our classifier was adequately prepared to detect stylistic features and fake art when trained alongside the generator in the GAN.</p>
        <p>Next we implemented the generator model. This model utilized transposed convolutions to take a small input of random inputs and a target style into a larger image fitting the input dimensions required by the discriminator: 3x256x256. We use matplotlib and Excel to graph and analyze the accuracy and values obtained from our Minimax Loss function. This will give us insight into parameters that can be manually changed to improve performance, and the general effectiveness of our neural network’s purposes. Ultimately, our metric of success is determined by the realism of the images produced by the generator model when tested against the human eye.</p>
        
        <h4>
            Results
        </h4>
        <p>First, we look at the quality of our discriminator. As seen in Figure 1 belows, after training over 15 epochs, we see that loss saw a consistent decline as the model progressed. Worried about potential overfitting, we utilized the validation dataset to determine how successful general classification was in the model.</p>
        <table>
            <tr>
                <img src="Discriminator1LossGraph.PNG" alt="Figure 1">
            </tr>
            <tr>
                <th>Figure 1</th>
            </tr>
        </table>
        <p>Relative to previous runs of the discriminator model in classification tests, this model design of seven convolutional layers and three linear layers (utilizing dropout) with a learning rate of .001 yielded the best results. Given the growth of accuracy and continued decline in loss, we set it up for use in training the GAN. That said, the highest accuracy categories in this set share one key feature. They are non-Western art forms. This could indicate strong similarity between the Western styles making them more difficult to distinguish relative to the other categories. We test stylistic similarity hypothesis by utilizing confusion matrices (Figure 2). We do the same during during GAN validaiton (Figure 3).</p>
        
        <table>
            <tr>
                <img src="DiscriminatorConfusionMatrix1.PNG" alt="Figure 2" style="width:650px;height:600px;">
            </tr>
            <tr>
                <th>Figure 2</th>
            </tr>
        </table>
        <p>ADD PARAGRAPH ABOUT GAN, GAN LOSS FLUCTUATING, ETC...</p>

        <h4>
            Discussion
        </h4>
        <p>Evaluating the performance of our model is relatively straightforward. We simply want to see how many fake/generated images classify as real/authentic works of art. Though this method sounds good in practice, it could ultimately be skewed by the generator model developing at a faster pace than the discriminator model. Since the goal is to make a model that can produce art in a given style, the similarity of the art pieces can also be determined by eye. That said, we continued to monitor the accuracy of images that are classified as real or fake to ensure that on epochs where the discriminator is backpropagated, it gets better over time, and on epochs where the generator is backpropagated the classifier gets worse.</p>
        <p>By evaluating these patterns on a per epoch basis we can also distinguish which/if one model is progressing significantly faster than the other, allowing us to determine if the learning rate of a given model could be reduced. Similarly, by comparing images generated to those from our dataset, we could visually evaluate the similarity to the dataset and consistency between generated images.</p>
        <p>FILLER PARAGRAPHS</p>

        <p>Finally, there are ethical implications to the art created by a machine that attributes no meaning to it. One of the styles we looked into was Kente, which is produced on textiles and which traditionally attributes meaning to different colors and patterns. Understanding this, we believe that even if the style is replicated well, it seems unethical to truly consider the work as Kente if its creator does not understand nor acknowledge the meanings of the components that create this style. In this sense, consideration of pAIcasso’s work, even if reproduced on a textile as authentic Kente would be an egregious misclassification. In addition, there are considerations for the effect of easily produced original art on small businesses and artists. Because artists put thought, time, and effort into their work, overproduction of art could hurt independent creators who can’t keep up with the expedited mechanical production.</p>

    </div>
    

</body>
</html>
